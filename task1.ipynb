{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12486024,"sourceType":"datasetVersion","datasetId":7878862},{"sourceId":13989132,"sourceType":"datasetVersion","datasetId":8916428}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nDATA_PATH = \"/kaggle/input/www2025-mmctr-data\"\nprint(\"Files in DATA_PATH:\", os.listdir(DATA_PATH))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:28:07.702168Z","iopub.execute_input":"2025-12-07T11:28:07.702386Z","iopub.status.idle":"2025-12-07T11:28:07.716554Z","shell.execute_reply.started":"2025-12-07T11:28:07.702364Z","shell.execute_reply":"2025-12-07T11:28:07.715944Z"}},"outputs":[{"name":"stdout","text":"Files in DATA_PATH: ['MicroLens_1M_MMCTR']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install protobuf==3.20.3\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-04T10:08:11.391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================\n# CTR_ItemEmb_Extraction_Kaggle.py\n# Extraction des embeddings texte+image, fusion + projection\n# ======================================================\n\nimport os\nimport torch\nimport torch.nn as nn\nimport polars as pl\nimport numpy as np\nfrom transformers import AutoProcessor, AutoModel\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\n# ---------------------------\n# CONFIG\n# ---------------------------\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = \"google/siglip-base-patch16-384\"\nSIGLIP_DIM = 768\nFINAL_DIM = 128\n\n# Répertoire racine du dataset\nBASE_DIR = \"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR\"\n\n# item_info est dans le sous-dossier MicroLens_1M_x1\nITEM_INFO_PATH = os.path.join(BASE_DIR, \"MicroLens_1M_x1\", \"item_info.parquet\")\n\n# Le fichier avec les titres\nITEM_FEATURE_PATH = os.path.join(BASE_DIR,\"item_feature.parquet\")\n\n# Les images sont dans item_images/item_images\nIMAGE_DIR = os.path.join(BASE_DIR, \"item_images\", \"item_images\")\n\n# Emplacement de sauvegarde\nSAVE_PATH = \"/kaggle/working/new_item_info.parquet\"\n\n# ---------------------------\n# FONCTION PRINCIPALE\n# ---------------------------\ndef main():\n    print(f\"Device: {DEVICE}\")\n    print(\"Chargement du modèle SigLIP...\")\n    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n    model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n    model.eval()\n    proj_layer = nn.Linear(SIGLIP_DIM*2, FINAL_DIM).to(DEVICE)\n    proj_layer.eval()\n\n    # Chargement item_info et item_feature\n    item_info = pl.read_parquet(ITEM_INFO_PATH)\n    item_feature = pl.read_parquet(ITEM_FEATURE_PATH)\n\n    # Merge pour avoir item_title\n    df = item_info.join(item_feature[['item_id', 'item_title']], on='item_id', how='left')\n\n    item_ids = df[\"item_id\"].to_list()\n    item_titles = df[\"item_title\"].to_list()\n    item_tags = df[\"item_tags\"].to_list()\n    print(f\"Chargé {len(item_ids)} items (padding inclus)\")\n\n    final_embs = []\n    missing = 0\n\n    # Boucle sur les items\n    for i, iid in tqdm(enumerate(item_ids), total=len(item_ids), desc=\"Extraction embeddings\"):\n        if iid == 0:  # padding\n            final_embs.append([0.0]*FINAL_DIM)\n            continue\n\n        # Récupère le titre réel\n        title = str(item_titles[i]) if item_titles[i] is not None else f\"item {iid}\"\n\n        img_path = os.path.join(IMAGE_DIR, f\"{iid}.jpg\")\n        if not os.path.exists(img_path):\n            missing += 1\n            final_embs.append([0.0]*FINAL_DIM)\n            continue\n\n        try:\n            # Embedding texte\n            text_inputs = processor(text=title, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n            with torch.no_grad():\n                text_emb = model.get_text_features(**text_inputs)\n                text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n\n            # Embedding image\n            image = Image.open(img_path).convert(\"RGB\")\n            img_inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n            with torch.no_grad():\n                img_emb = model.get_image_features(**img_inputs)\n                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n\n            # Fusion + projection\n            concat = torch.cat([text_emb, img_emb], dim=-1)\n            with torch.no_grad():\n                fused = proj_layer(concat).squeeze(0).cpu().numpy()\n                fused = fused / np.linalg.norm(fused)  # L2-normalization\n\n            final_embs.append(fused)\n\n        except Exception as e:\n            print(f\"Erreur pour item {iid}: {e}\")\n            final_embs.append([0.0]*FINAL_DIM)\n\n    print(f\"\\nExtraction terminée. Images manquantes : {missing}\")\n\n    # Sauvegarde\n    out_df = pl.DataFrame({\n      \"item_id\": item_ids,\n      \"item_tags\": item_tags,\n      \"item_emb_d128\": [np.array(v, dtype=np.float64).tolist() for v in final_embs]\n    }, strict=False)\n\n    out_df.write_parquet(SAVE_PATH)\n    print(f\"Fichier sauvegardé : {SAVE_PATH}\")\n    print(f\"Colonnes : item_id | item_tags | item_emb_d128\")\n\n# ---------------------------\n# EXECUTION\n# ---------------------------\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}